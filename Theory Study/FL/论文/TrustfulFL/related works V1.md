Federated Learning has become a crucial branch of traditional Machine Learning owing to its low communication cost and high equipment flexibility. In the federated learning protocols, users upload their local data vectors to the aggregation server. After the aggregation process, updated vectors will be distributed to each user for further training. This iterative and decentralized process allows federated learning to leverage insights from various data sources while preserving data privacy and security. It is particularly suitable for scenarios where centralized data collection is challenging or undesirable, such as edge computing, IoT devices, and privacy-sensitive applications. Compared with traditional centralized learning, federated learning minimizes the risk of leaking private information. At the same time, such a protocol also possesses robustness against training failures.

However, collecting a large amount of private information in federated learning can easily lead to security issues. Currently, most widely used federated learning models are faced with severe privacy protection deficiencies. Data poisoning is the malicious manipulation of local datasets by a participant to compromise the integrity of the global model. A dishonest participant may intentionally include misleading or corrupted data during local model training, aiming to weaken the overall security of the model. Such scenarios may consist of Dishonest clients/servers, Byzantine attacks, and Sybil attacks.

Aiming to solve these security issues, our work introduces a model that applies new methods of aggregation verification. The verification process can deduce one client's confidence level by an index that can evaluate the offset toward clean datasets to avoid training corruption due to malicious clients. A new signature generation algorithm is also imported to identify malicious servers and protect the raw datasets from being stolen.